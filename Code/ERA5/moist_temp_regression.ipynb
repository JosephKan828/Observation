{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5bc4c638",
   "metadata": {},
   "source": [
    "# Moisture and Temperature regressed with precipitation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e233bf5",
   "metadata": {},
   "source": [
    "## Import package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5475bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453634ef",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8836700a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [07:01<00:00, 23.42s/it]\n"
     ]
    }
   ],
   "source": [
    "# Load ERA5 data\n",
    "ERA5_PATH = \"/work/b11209013/2024_Research/ERA5/\"\n",
    "\n",
    "q = {}; t = {}; w = {}\n",
    "\n",
    "for central_lon in tqdm(range(0, 341, 20)):\n",
    "    \n",
    "    with xr.open_dataset(f\"{ERA5_PATH}q/q_sub.nc\", chunks={}) as f:\n",
    "        lon_centered = (f[\"lon\"] - central_lon + 180) % 360 - 180\n",
    "        f = f.assign_coords(lon=lon_centered).sortby(\"lon\")\n",
    "\n",
    "        q[str(central_lon)] = f[\"q\"].values*1000.0\n",
    "    \n",
    "    with xr.open_dataset(f\"{ERA5_PATH}t/t_sub.nc\", chunks={}) as f:\n",
    "        lon_centered = (f[\"lon\"] - central_lon + 180) % 360 - 180\n",
    "        f = f.assign_coords(lon=lon_centered).sortby(\"lon\")\n",
    "\n",
    "        t[str(central_lon)] = f[\"t\"].values\n",
    "\n",
    "    with xr.open_dataset(f\"{ERA5_PATH}w/w_Itp_sub.nc\", chunks={}) as f:\n",
    "        lon_centered = (f[\"lon\"] - central_lon + 180) % 360 - 180\n",
    "        f = f.assign_coords(lon=lon_centered).sortby(\"lon\")\n",
    "\n",
    "        w[str(central_lon)] = f[\"w\"].mean(dim=\"lat\").values\n",
    "\n",
    "# Load IMERG time series data\n",
    "IMERG_PATH = \"/home/b11209013/2025_Research/Obs/Files/IMERG/Hovmoller.h5\"\n",
    "\n",
    "Hov = {}\n",
    "\n",
    "with h5py.File(IMERG_PATH, \"r\") as f:\n",
    "    lon = np.array(f.get(\"lon\"))\n",
    "\n",
    "    hov_grp = f.get(\"precip\")\n",
    "\n",
    "    Hov = {key: np.array(hov_grp.get(key)) for key in hov_grp.keys()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4c0a79",
   "metadata": {},
   "source": [
    "## Compute regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48199bc6",
   "metadata": {},
   "source": [
    "### Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9be4b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def regression_slope(x, y):\n",
    "    \"\"\"Compute regression slope of y onto 1D x, vectorized over all grid points.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x : array_like, shape (nt,)\n",
    "        Predictor time series.\n",
    "    y : array_like, shape (nt, ...) \n",
    "        Predictand field with the same time dimension as x. Can be 2D, 3D, etc.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    slope : ndarray, shape y.shape[1:]\n",
    "        Regression slope at each grid point, with NaNs where regression is not defined.\n",
    "    \"\"\"\n",
    "    # Convert to arrays\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    y = np.asarray(y, dtype=float)\n",
    "\n",
    "    # Basic checks\n",
    "    if x.ndim != 1:\n",
    "        # Allow x to be (nt, 1, ..., 1) but collapse it\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        if x.shape[1] != 1:\n",
    "            raise ValueError(\"This implementation assumes x is a 1D time series (nt,).\")\n",
    "        x = x[:, 0]\n",
    "\n",
    "    if y.shape[0] != x.shape[0]:\n",
    "        raise ValueError(\"Time dimension of x and y must match (x.shape[0] == y.shape[0]).\")\n",
    "\n",
    "    nt = x.shape[0]\n",
    "    # Flatten spatial dims of y: (nt, npoints)\n",
    "    y_flat = y.reshape(nt, -1)                      # (nt, M)\n",
    "    M = y_flat.shape[1]\n",
    "\n",
    "    # Broadcast x to shape (nt, M)\n",
    "    x2d = x[:, None]                                # (nt, 1)\n",
    "\n",
    "    # Valid (non-NaN) mask per time & point\n",
    "    mask = ~np.isnan(y_flat) & ~np.isnan(x2d)       # (nt, M)\n",
    "\n",
    "    # Number of valid samples per point\n",
    "    n = np.sum(mask, axis=0)                        # (M,)\n",
    "\n",
    "    # Zero out invalid entries for sums\n",
    "    x_masked = np.where(mask, x2d, 0.0)             # (nt, M)\n",
    "    y_masked = np.where(mask, y_flat, 0.0)          # (nt, M)\n",
    "\n",
    "    # Sums over time for each grid point\n",
    "    sum_x  = np.sum(x_masked, axis=0)               # (M,)\n",
    "    sum_y  = np.sum(y_masked, axis=0)               # (M,)\n",
    "    sum_xx = np.sum(x_masked * x_masked, axis=0)    # (M,)\n",
    "    sum_xy = np.sum(x_masked * y_masked, axis=0)    # (M,)\n",
    "\n",
    "    # Closed-form slope per grid point\n",
    "    denom = n * sum_xx - sum_x**2\n",
    "    numer = n * sum_xy - sum_x * sum_y\n",
    "\n",
    "    slope_flat = np.full(M, np.nan, dtype=float)\n",
    "    valid_reg = (n > 1) & (denom != 0.0)\n",
    "    slope_flat[valid_reg] = numer[valid_reg] / denom[valid_reg]\n",
    "\n",
    "    # Back to original spatial shape\n",
    "    slope = slope_flat.reshape(y.shape[1:])\n",
    "    return slope\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548e41e5",
   "metadata": {},
   "source": [
    "### Compute regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7fd6ad74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing LW key 0: 100%|██████████| 7/7 [00:14<00:00,  2.03s/it]\n",
      "Processing LW key 20: 100%|██████████| 7/7 [00:14<00:00,  2.08s/it]\n",
      "Processing LW key 40: 100%|██████████| 7/7 [00:14<00:00,  2.11s/it]\n",
      "Processing LW key 60: 100%|██████████| 7/7 [00:14<00:00,  2.11s/it]\n",
      "Processing LW key 80: 100%|██████████| 7/7 [00:14<00:00,  2.05s/it]\n",
      "Processing LW key 100: 100%|██████████| 7/7 [00:14<00:00,  2.04s/it]\n",
      "Processing LW key 120: 100%|██████████| 7/7 [00:14<00:00,  2.03s/it]\n",
      "Processing LW key 140: 100%|██████████| 7/7 [00:14<00:00,  2.03s/it]\n",
      "Processing LW key 160: 100%|██████████| 7/7 [00:14<00:00,  2.07s/it]\n",
      "Processing LW key 180: 100%|██████████| 7/7 [00:14<00:00,  2.12s/it]\n",
      "Processing LW key 200: 100%|██████████| 7/7 [00:14<00:00,  2.10s/it]\n",
      "Processing LW key 220: 100%|██████████| 7/7 [00:14<00:00,  2.05s/it]\n",
      "Processing LW key 240: 100%|██████████| 7/7 [00:14<00:00,  2.05s/it]\n",
      "Processing LW key 260: 100%|██████████| 7/7 [00:14<00:00,  2.08s/it]\n",
      "Processing LW key 280: 100%|██████████| 7/7 [00:14<00:00,  2.04s/it]\n",
      "Processing LW key 300: 100%|██████████| 7/7 [00:14<00:00,  2.02s/it]\n",
      "Processing LW key 320: 100%|██████████| 7/7 [00:14<00:00,  2.02s/it]\n",
      "Processing LW key 340: 100%|██████████| 7/7 [00:14<00:00,  2.03s/it]\n"
     ]
    }
   ],
   "source": [
    "# Compute regression\n",
    "q_reg = {}\n",
    "t_reg = {}\n",
    "w_reg = {}\n",
    "\n",
    "for q_key in q.keys():\n",
    "    # preallocate dict\n",
    "    q_reg[str(q_key)] = {}\n",
    "    t_reg[str(q_key)] = {}\n",
    "    w_reg[str(q_key)] = {}\n",
    "\n",
    "    # find longitude index\n",
    "    lon_idx = np.argmin(np.abs(lon - (int(q_key))))\n",
    "\n",
    "    for hov_key in tqdm(Hov.keys(), desc=f\"Processing LW key {q_key}\"):\n",
    "        Hov_ts = Hov[hov_key][:, lon_idx]\n",
    "\n",
    "        q_reg[str(q_key)][str(hov_key)] = regression_slope(Hov_ts[:,None,None], q[q_key])\n",
    "        t_reg[str(q_key)][str(hov_key)] = regression_slope(Hov_ts[:,None,None], t[q_key])\n",
    "        w_reg[str(q_key)][str(hov_key)] = regression_slope(Hov_ts[:,None,None], w[q_key])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b145982",
   "metadata": {},
   "source": [
    "### Compute average values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e5e5fa4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_reg_composite = {\n",
    "    hov_key: np.nanmean(\n",
    "        np.array([q_reg[q_key][hov_key] for q_key in q.keys()]),\n",
    "        axis=0\n",
    "    )\n",
    "    for hov_key in Hov.keys()\n",
    "}\n",
    "\n",
    "t_reg_composite = {\n",
    "    hov_key: np.nanmean(\n",
    "        np.array([t_reg[t_key][hov_key] for t_key in t.keys()]),\n",
    "        axis=0\n",
    "    )\n",
    "    for hov_key in Hov.keys()\n",
    "}\n",
    "\n",
    "w_reg_composite = {\n",
    "    hov_key: np.nanmean(\n",
    "        np.array([w_reg[w_key][hov_key] for w_key in w.keys()]),\n",
    "        axis=0\n",
    "    )\n",
    "    for hov_key in Hov.keys()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af685543",
   "metadata": {},
   "source": [
    "## Save File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5437f7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(\"/work/b11209013/2025_Research/regression/IMERG_ERA5.h5\", \"w\") as f:\n",
    "    q_grp = f.create_group(\"q\")\n",
    "\n",
    "    for hov_key in Hov.keys():\n",
    "        q_grp_hov = q_grp.create_group(str(hov_key))\n",
    "        for q_key in q.keys():\n",
    "            q_grp_hov.create_dataset(str(q_key), data=np.array(q_reg[str(q_key)][str(hov_key)]))\n",
    "\n",
    "    t_grp = f.create_group(\"t\")\n",
    "\n",
    "    for hov_key in Hov.keys():\n",
    "        t_grp_hov = t_grp.create_group(str(hov_key))\n",
    "        for t_key in t.keys():\n",
    "            t_grp_hov.create_dataset(str(t_key), data=np.array(t_reg[str(t_key)][str(hov_key)]))\n",
    "\n",
    "    w_grp = f.create_group(\"w\")\n",
    "\n",
    "    for hov_key in Hov.keys():\n",
    "        w_grp_hov = w_grp.create_group(str(hov_key))\n",
    "        for w_key in w.keys():\n",
    "            w_grp_hov.create_dataset(str(w_key), data=np.array(w_reg[str(w_key)][str(hov_key)]))\n",
    "\n",
    "    q_comp_grp = f.create_group(\"q_composite\")\n",
    "\n",
    "    for hov_key in Hov.keys():\n",
    "        q_comp_grp.create_dataset(str(hov_key), data=np.array(q_reg_composite[str(hov_key)]))\n",
    "\n",
    "    t_comp_grp = f.create_group(\"t_composite\")\n",
    "\n",
    "    for hov_key in Hov.keys():\n",
    "        t_comp_grp.create_dataset(str(hov_key), data=np.array(t_reg_composite[str(hov_key)]))\n",
    "\n",
    "    w_comp_grp = f.create_group(\"w_composite\")\n",
    "\n",
    "    for hov_key in Hov.keys():\n",
    "        w_comp_grp.create_dataset(str(hov_key), data=np.array(w_reg_composite[str(hov_key)]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "atmo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
