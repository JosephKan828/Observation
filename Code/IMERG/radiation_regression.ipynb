{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5bc4c638",
   "metadata": {},
   "source": [
    "# Radiative heating regressed with precipitation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e233bf5",
   "metadata": {},
   "source": [
    "## Import package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f5475bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453634ef",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8836700a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CloudSat data\n",
    "CLOUDSAT_PATH = \"/work/b11209013/2025_Research/CloudSat/CloudSat_sub/\"\n",
    "\n",
    "lw = {}; sw = {}\n",
    "\n",
    "for central_lon in range(0, 341, 20):\n",
    "    \n",
    "    with xr.open_dataset(f\"{CLOUDSAT_PATH}qlw.nc\", chunks={}) as f:\n",
    "        lon_centered = (f[\"lon\"] - central_lon + 180) % 360 - 180\n",
    "        f = f.assign_coords(lon=lon_centered).sortby(\"lon\")\n",
    "\n",
    "        lw[str(central_lon)] = f[\"qlw\"].values\n",
    "    \n",
    "    with xr.open_dataset(f\"{CLOUDSAT_PATH}qsw.nc\", chunks={}) as f:\n",
    "        lon_centered = (f[\"lon\"] - central_lon + 180) % 360 - 180\n",
    "        f = f.assign_coords(lon=lon_centered).sortby(\"lon\")\n",
    "\n",
    "        sw[str(central_lon)] = f[\"qsw\"].values\n",
    "\n",
    "# Load IMERG time series data\n",
    "IMERG_PATH = \"/home/b11209013/2025_Research/Obs/Files/IMERG/Hovmoller.h5\"\n",
    "\n",
    "Hov = {}\n",
    "\n",
    "with h5py.File(IMERG_PATH, \"r\") as f:\n",
    "    lon = np.array(f.get(\"lon\"))\n",
    "\n",
    "    hov_grp = f.get(\"precip\")\n",
    "\n",
    "    Hov = {key: np.array(hov_grp.get(key)) for key in hov_grp.keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2f7dc083",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['kw_11_13', 'kw_1_3', 'kw_3_5', 'kw_5_7', 'kw_7_9', 'kw_9_11', 'mjo_1_4'])\n"
     ]
    }
   ],
   "source": [
    "print(Hov.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4c0a79",
   "metadata": {},
   "source": [
    "## Compute regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48199bc6",
   "metadata": {},
   "source": [
    "### Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d9be4b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def regression_slope(x, y):\n",
    "    \"\"\"Compute regression slope of y onto 1D x, vectorized over all grid points.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x : array_like, shape (nt,)\n",
    "        Predictor time series.\n",
    "    y : array_like, shape (nt, ...) \n",
    "        Predictand field with the same time dimension as x. Can be 2D, 3D, etc.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    slope : ndarray, shape y.shape[1:]\n",
    "        Regression slope at each grid point, with NaNs where regression is not defined.\n",
    "    \"\"\"\n",
    "    # Convert to arrays\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    y = np.asarray(y, dtype=float)\n",
    "\n",
    "    # Basic checks\n",
    "    if x.ndim != 1:\n",
    "        # Allow x to be (nt, 1, ..., 1) but collapse it\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        if x.shape[1] != 1:\n",
    "            raise ValueError(\"This implementation assumes x is a 1D time series (nt,).\")\n",
    "        x = x[:, 0]\n",
    "\n",
    "    if y.shape[0] != x.shape[0]:\n",
    "        raise ValueError(\"Time dimension of x and y must match (x.shape[0] == y.shape[0]).\")\n",
    "\n",
    "    nt = x.shape[0]\n",
    "    # Flatten spatial dims of y: (nt, npoints)\n",
    "    y_flat = y.reshape(nt, -1)                      # (nt, M)\n",
    "    M = y_flat.shape[1]\n",
    "\n",
    "    # Broadcast x to shape (nt, M)\n",
    "    x2d = x[:, None]                                # (nt, 1)\n",
    "\n",
    "    # Valid (non-NaN) mask per time & point\n",
    "    mask = ~np.isnan(y_flat) & ~np.isnan(x2d)       # (nt, M)\n",
    "\n",
    "    # Number of valid samples per point\n",
    "    n = np.sum(mask, axis=0)                        # (M,)\n",
    "\n",
    "    # Zero out invalid entries for sums\n",
    "    x_masked = np.where(mask, x2d, 0.0)             # (nt, M)\n",
    "    y_masked = np.where(mask, y_flat, 0.0)          # (nt, M)\n",
    "\n",
    "    # Sums over time for each grid point\n",
    "    sum_x  = np.sum(x_masked, axis=0)               # (M,)\n",
    "    sum_y  = np.sum(y_masked, axis=0)               # (M,)\n",
    "    sum_xx = np.sum(x_masked * x_masked, axis=0)    # (M,)\n",
    "    sum_xy = np.sum(x_masked * y_masked, axis=0)    # (M,)\n",
    "\n",
    "    # Closed-form slope per grid point\n",
    "    denom = n * sum_xx - sum_x**2\n",
    "    numer = n * sum_xy - sum_x * sum_y\n",
    "\n",
    "    slope_flat = np.full(M, np.nan, dtype=float)\n",
    "    valid_reg = (n > 1) & (denom != 0.0)\n",
    "    slope_flat[valid_reg] = numer[valid_reg] / denom[valid_reg]\n",
    "\n",
    "    # Back to original spatial shape\n",
    "    slope = slope_flat.reshape(y.shape[1:])\n",
    "    return slope\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548e41e5",
   "metadata": {},
   "source": [
    "### Compute regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7fd6ad74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing LW key 0: 100%|██████████| 7/7 [00:10<00:00,  1.45s/it]\n",
      "Processing LW key 20: 100%|██████████| 7/7 [00:10<00:00,  1.44s/it]\n",
      "Processing LW key 40: 100%|██████████| 7/7 [00:10<00:00,  1.44s/it]\n",
      "Processing LW key 60: 100%|██████████| 7/7 [00:10<00:00,  1.44s/it]\n",
      "Processing LW key 80: 100%|██████████| 7/7 [00:10<00:00,  1.44s/it]\n",
      "Processing LW key 100: 100%|██████████| 7/7 [00:10<00:00,  1.44s/it]\n",
      "Processing LW key 120: 100%|██████████| 7/7 [00:10<00:00,  1.45s/it]\n",
      "Processing LW key 140: 100%|██████████| 7/7 [00:10<00:00,  1.45s/it]\n",
      "Processing LW key 160: 100%|██████████| 7/7 [00:10<00:00,  1.45s/it]\n",
      "Processing LW key 180: 100%|██████████| 7/7 [00:10<00:00,  1.46s/it]\n",
      "Processing LW key 200: 100%|██████████| 7/7 [00:10<00:00,  1.46s/it]\n",
      "Processing LW key 220: 100%|██████████| 7/7 [00:10<00:00,  1.45s/it]\n",
      "Processing LW key 240: 100%|██████████| 7/7 [00:10<00:00,  1.47s/it]\n",
      "Processing LW key 260: 100%|██████████| 7/7 [00:10<00:00,  1.45s/it]\n",
      "Processing LW key 280: 100%|██████████| 7/7 [00:10<00:00,  1.47s/it]\n",
      "Processing LW key 300: 100%|██████████| 7/7 [00:10<00:00,  1.47s/it]\n",
      "Processing LW key 320: 100%|██████████| 7/7 [00:10<00:00,  1.45s/it]\n",
      "Processing LW key 340: 100%|██████████| 7/7 [00:10<00:00,  1.47s/it]\n"
     ]
    }
   ],
   "source": [
    "# set cross iters\n",
    "cross_iters = list(product(lw.keys(), Hov.keys()))\n",
    "\n",
    "# Compute regression\n",
    "lw_reg = {}\n",
    "sw_reg = {}\n",
    "\n",
    "for lw_key in lw.keys():\n",
    "    # preallocate dict\n",
    "    lw_reg[str(lw_key)] = {}\n",
    "    sw_reg[str(lw_key)] = {}\n",
    "\n",
    "    # find longitude index\n",
    "    lon_idx = np.argmin(np.abs(lon - (int(lw_key))))\n",
    "\n",
    "    for hov_key in tqdm(Hov.keys(), desc=f\"Processing LW key {lw_key}\"):\n",
    "        Hov_ts = Hov[hov_key][:, lon_idx]\n",
    "\n",
    "        lw_reg[str(lw_key)][str(hov_key)] = regression_slope(Hov_ts[:,None,None], lw[lw_key])\n",
    "        sw_reg[str(lw_key)][str(hov_key)] = regression_slope(Hov_ts[:,None,None], sw[lw_key])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b145982",
   "metadata": {},
   "source": [
    "### Compute average values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e5e5fa4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lw_reg_composite = {\n",
    "    hov_key: np.nanmean(\n",
    "        np.array([lw_reg[lw_key][hov_key] for lw_key in lw.keys()]),\n",
    "        axis=0\n",
    "    )\n",
    "    for hov_key in Hov.keys()\n",
    "}\n",
    "\n",
    "sw_reg_composite = {\n",
    "    hov_key: np.nanmean(\n",
    "        np.array([sw_reg[lw_key][hov_key] for lw_key in lw.keys()]),\n",
    "        axis=0\n",
    "    )\n",
    "    for hov_key in Hov.keys()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af685543",
   "metadata": {},
   "source": [
    "## Save File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5437f7b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.00488785 -0.00319218 -0.0218375  ... -0.0135173  -0.02385384\n",
      "  -0.02401111]\n",
      " [-0.0059989   0.00460006 -0.0165255  ...  0.02055635  0.01549107\n",
      "   0.00964484]\n",
      " [-0.00544915  0.00278226 -0.01813823 ...  0.00576117  0.00980284\n",
      "  -0.00925189]\n",
      " ...\n",
      " [ 0.01959025  0.01597927  0.03679337 ...  0.01454462  0.01751469\n",
      "   0.00748313]\n",
      " [ 0.01065093  0.0064921   0.01521761 ...  0.01624153  0.01237952\n",
      "   0.00653634]\n",
      " [ 0.00107868  0.00076695 -0.00044444 ...  0.00499815  0.00224997\n",
      "   0.00394667]]\n",
      "[[-0.02083491 -0.01164966  0.01877236 ...  0.00966246 -0.00228353\n",
      "  -0.02459476]\n",
      " [ 0.0058368   0.01009616  0.01816164 ...  0.01613493 -0.00321765\n",
      "   0.00671684]\n",
      " [-0.00289956  0.00978028  0.02039129 ...  0.01392519 -0.01058076\n",
      "   0.00312892]\n",
      " ...\n",
      " [-0.00329654 -0.00157473 -0.0038394  ... -0.0214445  -0.00028398\n",
      "  -0.00620511]\n",
      " [-0.00380835 -0.00604689 -0.00174176 ... -0.01118916 -0.00712596\n",
      "  -0.01113752]\n",
      " [-0.00399349 -0.00116051  0.0017137  ... -0.00526984 -0.00505238\n",
      "  -0.00755261]]\n",
      "[[-0.02884815 -0.02245503 -0.00749668 ... -0.02000758 -0.01254258\n",
      "  -0.019374  ]\n",
      " [-0.01092516 -0.00405755  0.01268381 ... -0.01840243 -0.01735502\n",
      "  -0.00563345]\n",
      " [-0.01494984 -0.00331505  0.00897194 ... -0.01940611 -0.01147752\n",
      "  -0.01217185]\n",
      " ...\n",
      " [ 0.01086303  0.00748591  0.00250379 ...  0.01309647  0.00297045\n",
      "   0.0017264 ]\n",
      " [ 0.00011698 -0.00536314 -0.0006589  ...  0.00354912 -0.00224871\n",
      "  -0.00573921]\n",
      " [ 0.00327095  0.00371416  0.0063698  ...  0.0042311   0.00084954\n",
      "   0.00096251]]\n",
      "[[-0.02189531 -0.00229939 -0.02108832 ... -0.05439008 -0.02055951\n",
      "  -0.00213395]\n",
      " [-0.01635538  0.00438467  0.00256886 ... -0.01516044 -0.02018963\n",
      "  -0.00880072]\n",
      " [-0.00358299  0.01449495 -0.0019309  ... -0.0186385  -0.03214497\n",
      "  -0.01279178]\n",
      " ...\n",
      " [ 0.01473379  0.01006942  0.01559901 ...  0.01511617 -0.00098376\n",
      "   0.0024477 ]\n",
      " [-0.00253135  0.00054438  0.00946141 ...  0.00305843 -0.00085774\n",
      "  -0.00420816]\n",
      " [ 0.00321442 -0.00043601  0.00227154 ...  0.00099158  0.00044458\n",
      "  -0.00036427]]\n",
      "[[-0.05223078 -0.02082578  0.00165014 ... -0.05220057 -0.02439407\n",
      "  -0.0261231 ]\n",
      " [-0.03652356 -0.01498618 -0.0073711  ... -0.02397698 -0.01189267\n",
      "  -0.01253967]\n",
      " [-0.02348861  0.00377552 -0.00734398 ... -0.02447026 -0.0056212\n",
      "  -0.00227893]\n",
      " ...\n",
      " [ 0.01463235  0.01921769  0.01331915 ...  0.02266528  0.00978209\n",
      "   0.01092354]\n",
      " [ 0.00278641  0.01207883  0.00854185 ...  0.00344994  0.00094292\n",
      "  -0.00020322]\n",
      " [ 0.0031393   0.00175256  0.00189949 ...  0.0036226   0.00252291\n",
      "   0.00034724]]\n",
      "[[-0.04523724 -0.01381462 -0.0101876  ... -0.0246404  -0.00637431\n",
      "  -0.01075579]\n",
      " [-0.03055051  0.00305531 -0.00930077 ... -0.0063508  -0.00248671\n",
      "  -0.03678667]\n",
      " [-0.01091467  0.00032081 -0.00464321 ...  0.00055478 -0.00232091\n",
      "  -0.0262383 ]\n",
      " ...\n",
      " [ 0.0239506   0.02779444  0.02155414 ...  0.0329103   0.01991045\n",
      "   0.01469596]\n",
      " [ 0.01437518  0.00717126  0.00589697 ...  0.01759013  0.01815087\n",
      "   0.00738839]\n",
      " [ 0.00951938  0.00576824  0.00427208 ...  0.00639926  0.01086714\n",
      "   0.00657693]]\n",
      "[[-0.04260783 -0.03442774 -0.00217667 ... -0.01131556 -0.01766395\n",
      "  -0.02829303]\n",
      " [-0.0410963  -0.03283348 -0.02801704 ... -0.02235484 -0.01675699\n",
      "  -0.03111137]\n",
      " [-0.06237964 -0.06193894 -0.03960356 ... -0.02658247 -0.03972513\n",
      "  -0.04840138]\n",
      " ...\n",
      " [ 0.02284841  0.00524246  0.01164024 ...  0.00904083  0.0085042\n",
      "   0.01096987]\n",
      " [ 0.00724481 -0.00456879 -0.0018047  ...  0.0031633   0.00423137\n",
      "   0.00011568]\n",
      " [-0.00441954 -0.01114241 -0.00385494 ... -0.00098526 -0.00582215\n",
      "  -0.00445472]]\n"
     ]
    }
   ],
   "source": [
    "with h5py.File(\"/work/b11209013/2025_Research/regression/IMERG_CLOUDSAT.h5\", \"w\") as f:\n",
    "    lw_grp = f.create_group(\"lw\")\n",
    "\n",
    "    for hov_key in Hov.keys():\n",
    "        lw_grp_hov = lw_grp.create_group(str(hov_key))\n",
    "        for lw_key in lw.keys():\n",
    "            lw_grp_hov.create_dataset(str(lw_key), data=np.array(lw_reg[str(lw_key)][str(hov_key)]))\n",
    "\n",
    "    sw_grp = f.create_group(\"sw\")\n",
    "\n",
    "    for hov_key in Hov.keys():\n",
    "        sw_grp_hov = sw_grp.create_group(str(hov_key))\n",
    "        for sw_key in sw.keys():\n",
    "            sw_grp_hov.create_dataset(str(sw_key), data=np.array(sw_reg[str(sw_key)][str(hov_key)]))\n",
    "\n",
    "    lw_comp_grp = f.create_group(\"lw_composite\")\n",
    "\n",
    "    for hov_key in Hov.keys():\n",
    "        lw_comp_grp.create_dataset(str(hov_key), data=np.array(lw_reg_composite[str(hov_key)]))\n",
    "\n",
    "    sw_comp_grp = f.create_group(\"sw_composite\")\n",
    "\n",
    "    for hov_key in Hov.keys():\n",
    "        sw_comp_grp.create_dataset(str(hov_key), data=np.array(sw_reg_composite[str(hov_key)]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "atmo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
